# modules/ai/router.py
"""
AI Brain Router for Syntax Prime V2 - DEBUGGED VERSION
Handles all AI endpoints including chat with full integration support
Integration Order: Weather â†’ Bluesky â†’ RSS â†’ Scraper â†’ Prayer â†’ Google Trends â†’ Voice â†’ Image â†’ Health â†’ Chat/AI
Date: 9/27/25 - Added extensive debugging and fixed critical bugs
Date: 9/27/25 - Added prayer notifications, location detection, and Google Trends integration
Date: 9/28/25 - Added Voice Synthesis and Image Generation to integration chain
"""

import asyncio
import uuid
import time
from datetime import datetime
from typing import Dict, List, Optional, Any
import json
import logging
import os

from fastapi import APIRouter, HTTPException, Depends, Request,Form, UploadFile,File
from pydantic import BaseModel, Field
from typing import Dict, List, Optional, Any

# Import our AI brain components
from .openrouter_client import get_openrouter_client, cleanup_openrouter_client
from .inception_client import get_inception_client, cleanup_inception_client
from .conversation_manager import get_memory_manager, cleanup_memory_managers
from .knowledge_query import get_knowledge_engine
from .personality_engine import get_personality_engine
from .feedback_processor import get_feedback_processor

logger = logging.getLogger(__name__)

#-- Request/Response Models
class ChatRequest(BaseModel):
    message: str = Field(..., description="User's message")
    personality_id: str = Field(default='syntaxprime', description="AI personality to use")
    thread_id: Optional[str] = Field(None, description="Conversation thread ID")
    include_knowledge: bool = Field(default=True, description="Include knowledge base search")
    stream: bool = Field(default=False, description="Stream response")

class ChatResponse(BaseModel):
    response: str
    thread_id: str
    message_id: str
    personality_id: str
    model_used: str
    response_time_ms: int
    knowledge_sources: List[Dict] = []
    conversation_context: Dict = {}

class FeedbackRequest(BaseModel):
    message_id: str = Field(..., description="Message ID to rate")
    feedback_type: str = Field(..., description="good, bad, or personality")
    feedback_text: Optional[str] = Field(None, description="Optional text feedback")

class FeedbackResponse(BaseModel):
    feedback_id: str
    feedback_type: str
    emoji: str
    learning_result: Dict
    message: str

#-- Router Setup
router = APIRouter(prefix="/ai", tags=["ai"])
DEFAULT_USER_ID = "b7c60682-4815-4d9d-8ebe-66c6cd24eff9"

# Dependency function
async def get_current_user_id() -> str:
    """Get current user ID - placeholder for now"""
    return DEFAULT_USER_ID

#-- Main Chat Endpoint (THE CORE FUNCTIONALITY WITH EXTENSIVE DEBUG)
@router.post("/chat", response_model=ChatResponse)
async def chat_with_ai(
    message: str = Form(...),
    personality_id: str = Form(default='syntaxprime'),
    thread_id: Optional[str] = Form(None),
    include_knowledge: bool = Form(default=True),
    files: List[UploadFile] = File(default=[]),
    request: Request = None,
    user_id: str = Depends(get_current_user_id)
):
    """
    Main chat endpoint with file upload support
    Integration Order: Weather â†’ Bluesky â†’ RSS â†’ Scraper â†’ Prayer â†’ Google Trends â†’ Voice â†’ Image â†’ Health â†’ Chat/AI
    """
    # Process uploaded files if any
    file_context = ""
    if files and len(files) > 0:
        logger.info(f"ðŸ“Ž Processing {len(files)} uploaded files")
        from .chat import process_uploaded_files
        
        try:
            processed_files = await process_uploaded_files(files)
            logger.info(f"âœ… Files processed successfully: {[f['filename'] for f in processed_files]}")
            
            # Add file context to the message
            file_context = "\n\n**Uploaded Files:**\n"
            for file_info in processed_files:
                file_context += f"\nðŸ“Ž **{file_info['filename']}**\n"
                if file_info['analysis'].get('description'):
                    file_context += f"   {file_info['analysis']['description']}\n"
        except Exception as e:
            logger.error(f"âŒ File processing failed: {e}")
            file_context = f"\n\nâš ï¸ Note: Some files could not be processed: {str(e)}"
    
    # Combine message with file context
    full_message = message + file_context
    
    start_time = time.time()
    thread_id = thread_id or str(uuid.uuid4())
    
    # ðŸš¨ CRITICAL DEBUG: Log everything
    logger.info(f"ðŸš€ CHAT START: Processing message '{message[:50]}...'")
    logger.info(f"ðŸ” DEBUG: user_id={user_id}, personality={personality_id}")
    logger.info(f"ðŸ§µ DEBUG: thread_id={thread_id}")
    
    try:
        # Import helper functions from chat.py
        logger.info("ðŸ“¦ DEBUG: Loading helper functions from chat.py...")
        try:
            from .chat import (
                 get_current_datetime_context,
                 detect_weather_request, get_weather_for_user,
                 detect_prayer_command, process_prayer_command,
                 detect_prayer_notification_command, process_prayer_notification_command,
                 detect_location_command, process_location_command,
                 detect_bluesky_command, process_bluesky_command,
                 detect_rss_command, get_rss_marketing_context,
                 detect_scraper_command, process_scraper_command,
                 detect_trends_command, process_trends_command,
                 detect_voice_command, process_voice_command,
                 detect_image_command, process_image_command,
                 detect_google_command, process_google_command,
                 detect_pattern_complaint, handle_pattern_complaint,
                 detect_email_detail_command, process_email_detail_command,
                 detect_draft_creation_command, process_draft_creation_command # NEW 10/2/25
            )
            logger.info("âœ… DEBUG: All chat helper functions loaded successfully")
        except Exception as e:
            logger.error(f"âŒ DEBUG: Failed to import chat helpers: {e}")
            raise
        
        # Get AI components
        logger.info("ðŸ§  DEBUG: Loading AI components...")
        try:
            personality_engine = get_personality_engine()
            memory_manager = get_memory_manager(user_id)
            knowledge_engine = get_knowledge_engine()
            openrouter_client = await get_openrouter_client()
            logger.info("âœ… DEBUG: All AI components loaded successfully")
        except Exception as e:
            logger.error(f"âŒ DEBUG: Failed to load AI components: {e}")
            raise
        
        # Get current datetime context
        logger.info("ðŸ“… DEBUG: Getting datetime context...")
        try:
            datetime_context = get_current_datetime_context()
            logger.info(f"âœ… DEBUG: Datetime context: {datetime_context['full_datetime']}")
        except Exception as e:
            logger.error(f"âŒ DEBUG: Failed to get datetime context: {e}")
            datetime_context = {"full_datetime": "Unknown time"}
        
        # Ensure thread exists or create it
        logger.info("ðŸ§µ DEBUG: Managing conversation thread...")
        try:
            if not thread_id:
                logger.info("ðŸ†• DEBUG: Creating new thread...")
                thread_id = await memory_manager.create_conversation_thread(
                    platform='web',
                    title=None  # Will be auto-generated from first message
                )
                logger.info(f"âœ… DEBUG: New thread created: {thread_id}")
            else:
                # Verify existing thread exists
                logger.info(f"ðŸ” DEBUG: Verifying existing thread: {thread_id}")
                try:
                    from ..core.database import db_manager
                    thread_check = await db_manager.fetch_one(
                        "SELECT id FROM conversation_threads WHERE id = $1 AND user_id = $2",
                        thread_id, user_id
                    )
                    if not thread_check:
                        logger.warning(f"âš ï¸ DEBUG: Thread {thread_id} not found, creating new thread")
                        thread_id = await memory_manager.create_conversation_thread(
                            platform='web',
                            title=None
                        )
                        logger.info(f"âœ… DEBUG: Fallback thread created: {thread_id}")
                    else:
                        thread_id = thread_id
                        logger.info(f"âœ… DEBUG: Using existing thread: {thread_id}")
                except Exception as e:
                    logger.error(f"âŒ DEBUG: Error checking thread: {e}")
                    # Create new thread as fallback
                    thread_id = await memory_manager.create_conversation_thread(
                        platform='web',
                        title=None
                    )
                    logger.info(f"âœ… DEBUG: Emergency fallback thread created: {thread_id}")
        except Exception as e:
            logger.error(f"âŒ DEBUG: Thread management failed: {e}")
            raise
        
        # Store user message
        logger.info("ðŸ’¾ DEBUG: Storing user message...")
        try:
            user_message_id = await memory_manager.add_message(
                thread_id=thread_id,
                role="user",
                content=message
            )
            logger.info(f"âœ… DEBUG: User message stored: {user_message_id}")
        except Exception as e:
            logger.error(f"âŒ DEBUG: Failed to store user message: {e}")
            raise
        
        # Build message content
        message_content = message
        logger.info(f"ðŸ” DEBUG: Processing message content: '{message_content[:100]}...'")
        
        # INTEGRATION ORDER: Check for special commands in the specified order
        special_response = None
        model_used = "integration_response"
        knowledge_sources = []
        
        # Initialize context_info for special responses (BUG FIX #1)
        context_info = {
            'total_messages': 1,
            'estimated_tokens': 0,
            'has_memory_context': False
        }
        
        logger.info("ðŸ” DEBUG: Starting integration command detection...")
        
        # 1. ðŸŒ¦ï¸ Weather command detection (FIRST)
        logger.info("ðŸŒ¦ï¸ DEBUG: Checking for weather commands...")
        if detect_weather_request(message_content):
            logger.info("âœ… DEBUG: Weather command detected - processing...")
            try:
                weather_data = await get_weather_for_user(user_id)
                logger.info(f"ðŸ“Š DEBUG: Weather data received: {weather_data.get('success', False)}")
                
                if weather_data.get('success'):
                    weather_info = weather_data['data']
                    special_response = f"""ðŸŒ¦ï¸ **Current Weather**

ðŸ“ **Location:** {weather_info.get('location', 'Unknown')}
ðŸŒ¡ï¸ **Temperature:** {weather_info.get('temperature_f', 'N/A')}Â°F
ðŸ’§ **Humidity:** {weather_info.get('humidity', 'N/A')}%
ðŸ’¨ **Wind:** {weather_info.get('wind_speed', 'N/A')} mph
ðŸ“Š **Pressure:** {weather_info.get('pressure', 'N/A')} mbar
â˜€ï¸ **UV Index:** {weather_info.get('uv_index', 'N/A')}
ðŸ‘ï¸ **Visibility:** {weather_info.get('visibility', 'N/A')} miles

Weather data powered by Tomorrow.io"""
                    logger.info("âœ… DEBUG: Weather response generated successfully")
                else:
                    special_response = f"ðŸŒ¦ï¸ **Weather Service Error**\n\nUnable to retrieve weather data: {weather_data.get('error', 'Unknown error')}"
                    logger.warning(f"âš ï¸ DEBUG: Weather service error: {weather_data.get('error')}")
            except Exception as e:
                logger.error(f"âŒ DEBUG: Weather processing failed: {e}")
                special_response = f"ðŸŒ¦ï¸ **Weather Processing Error**\n\nError: {str(e)}"
        
        # 2. ðŸ”µ Bluesky command detection (SECOND)
        elif detect_bluesky_command(message_content):
            logger.info("ðŸ”µ DEBUG: Bluesky command detected - processing...")
            try:
                # Import directly to avoid cache issues
                from ..integrations.bluesky.multi_account_client import get_bluesky_multi_client
                
                multi_client = get_bluesky_multi_client()
                message_lower = message_content.lower()
                
                if 'health' in message_lower:
                    try:
                        auth_results = await multi_client.authenticate_all_accounts()
                        working_count = sum(auth_results.values())
                        total_count = len(auth_results)
                        
                        special_response = f"""ðŸ”µ **Bluesky System Health**

ðŸ“± **Accounts:** {working_count}/{total_count} connected
ðŸ¤– **AI Assistant:** All personalities loaded
âš™ï¸ **Status:** {'Healthy' if working_count > 0 else 'Needs Attention'}"""
                        
                    except Exception as e:
                        special_response = f"âŒ **Health Check Failed:** {str(e)}"
                else:
                    # Use direct method call
                    account_statuses = multi_client.get_all_accounts_status()
                    special_response = f"""ðŸ”µ **Bluesky Social Media Intelligence**

ðŸ“± **Configured Accounts:** {len(account_statuses)}/5
ðŸ¤– **Features:** Keyword intelligence, engagement suggestions, approval workflow

**Available Commands:**
- `bluesky health` - System health check
- `bluesky accounts` - Check account status"""
                
                logger.info("âœ… DEBUG: Bluesky response generated successfully")
            except Exception as e:
                logger.error(f"âŒ DEBUG: Bluesky processing failed: {e}")
                special_response = f"ðŸ”µ **Bluesky Processing Error**\n\nError: {str(e)}"
        
        # 3. ðŸ“° RSS Learning command detection (THIRD)
        elif detect_rss_command(message_content):
            logger.info("ðŸ“° DEBUG: RSS Learning command detected - processing...")
            # RSS is handled differently - it provides context rather than direct responses
            # We'll get the context and let the AI incorporate it naturally
            try:
                rss_context = await get_rss_marketing_context(message_content)
                logger.info("âœ… DEBUG: RSS context retrieved, will process in AI section")
            except Exception as e:
                logger.error(f"âŒ DEBUG: RSS context retrieval failed: {e}")
        
        # 4. ðŸ” Marketing Scraper command detection (FOURTH)
        elif detect_scraper_command(message_content):
            logger.info("ðŸ” DEBUG: Marketing scraper command detected - processing...")
            try:
                special_response = await process_scraper_command(message_content, user_id)
                logger.info("âœ… DEBUG: Scraper response generated successfully")
            except Exception as e:
                logger.error(f"âŒ DEBUG: Scraper processing failed: {e}")
                special_response = f"ðŸ” **Scraper Processing Error**\n\nError: {str(e)}"
        
        # 5. ðŸ•Œ Prayer Times command detection (FIFTH) - with IP location
        elif detect_prayer_command(message_content):
            logger.info("ðŸ•Œ DEBUG: Prayer times command detected - processing with IP location...")
            try:
                # Get client IP address for location detection
                client_ip = request.client.host if hasattr(request, 'client') and request.client else None
                logger.info(f"ðŸŒ DEBUG: Using IP address for location: {client_ip}")
                
                special_response = await process_prayer_command(message_content, user_id, client_ip)
                logger.info("âœ… DEBUG: Prayer times response generated successfully")
            except Exception as e:
                logger.error(f"âŒ DEBUG: Prayer times processing failed: {e}")
                special_response = f"ðŸ•Œ **Prayer Times Processing Error**\n\nError: {str(e)}"
        
        # 5.1 ðŸ”” Prayer Notification Management (FIFTH-A) - with IP location
        elif detect_prayer_notification_command(message_content):
            logger.info("ðŸ”” DEBUG: Prayer notification command detected")
            try:
                client_ip = request.client.host if hasattr(request, 'client') and request.client else None
                special_response = await process_prayer_notification_command(message_content, user_id, client_ip)
                logger.info("âœ… DEBUG: Prayer notification command response generated successfully")
            except Exception as e:
                logger.error(f"âŒ DEBUG: Prayer notification command processing failed: {e}")
                special_response = f"ðŸ”” **Prayer Notification Error**\n\nUnable to process notification request: {str(e)}"
        
        # 5.2 ðŸ“ Location Detection Commands (FIFTH-B)
        elif detect_location_command(message_content):
            logger.info("ðŸ“ DEBUG: Location command detected")
            try:
                client_ip = request.client.host if hasattr(request, 'client') and request.client else None
                special_response = await process_location_command(message_content, user_id, client_ip)
                logger.info("âœ… DEBUG: Location command response generated successfully")
            except Exception as e:
                logger.error(f"âŒ DEBUG: Location command processing failed: {e}")
                special_response = f"ðŸ“ **Location Detection Error**\n\nUnable to process location request: {str(e)}"
        
        # 5.3 ðŸš« Pattern Fatigue Complaints (FIFTH-C)
        elif detect_pattern_complaint(message_content)[0]:
            logger.info("ðŸš« DEBUG: Pattern complaint detected")
            try:
                is_complaint, pattern_type, complaint_text = detect_pattern_complaint(message_content)
                special_response = await handle_pattern_complaint(user_id, pattern_type, complaint_text)
                logger.info("âœ… DEBUG: Pattern complaint handled successfully")
            except Exception as e:
                logger.error(f"âŒ DEBUG: Pattern complaint handling failed: {e}")
                special_response = "I understand you want me to stop that pattern. I'll try to be less repetitive."
        
        # 6. ðŸ“ˆ Google Trends command detection (SIXTH)
        elif detect_trends_command(message_content)[0]:  # [0] gets the boolean from the tuple
            logger.info("ðŸ“ˆ DEBUG: Google Trends command detected - processing...")
            try:
                special_response = await process_trends_command(message_content, user_id)
                logger.info("âœ… DEBUG: Google Trends response generated successfully")
            except Exception as e:
                logger.error(f"âŒ DEBUG: Google Trends processing failed: {e}")
                special_response = f"ðŸ“ˆ **Google Trends Processing Error**\n\nError: {str(e)}"
        
        # 7. ðŸŽ¤ Voice Synthesis command detection (SEVENTH) - NEW 9/28/25
        elif detect_voice_command(message_content):
            logger.info("ðŸŽ¤ DEBUG: Voice synthesis command detected - processing...")
            try:
                special_response = await process_voice_command(message_content, user_id)
                logger.info("âœ… DEBUG: Voice synthesis response generated successfully")
            except Exception as e:
                logger.error(f"âŒ DEBUG: Voice synthesis processing failed: {e}")
                special_response = f"ðŸŽ¤ **Voice Synthesis Processing Error**\n\nError: {str(e)}"
        
        # 8. ðŸŽ¨ Image Generation command detection (EIGHTH) - NEW 9/28/25
        elif detect_image_command(message_content):
            logger.info("ðŸŽ¨ DEBUG: Image generation command detected - processing...")
            try:
                special_response = await process_image_command(message_content, user_id)
                logger.info("âœ… DEBUG: Image generation response generated successfully")
            except Exception as e:
                logger.error(f"âŒ DEBUG: Image generation processing failed: {e}")
                special_response = f"ðŸŽ¨ **Image Generation Processing Error**\n\nError: {str(e)}"
        
       # 9a. ðŸ“§ Email Detail & Draft Commands - NEW 10/2/25
       # 9a. ðŸ“§ Email Detail & Draft Commands (EIGHTH) - MOVED BEFORE IMAGE - 10/3/25
        elif detect_email_detail_command(message_content)[0] or detect_draft_creation_command(message_content)[0]:
            logger.info("ðŸ“§ DEBUG: Email or draft command detected - determining type...")
            
            # Get the actual detection results
            is_email_cmd, action_type, email_num = detect_email_detail_command(message_content)
            is_draft_cmd, draft_email_num, draft_instruction = detect_draft_creation_command(message_content)
            
            if is_email_cmd:
                logger.info(f"ðŸ“§ DEBUG: Email detail command detected: {action_type} for email #{email_num}")
                try:
                    special_response = await process_email_detail_command(action_type, email_num, user_id)
                    logger.info("âœ… DEBUG: Email detail response generated successfully")
                except Exception as e:
                    logger.error(f"âŒ DEBUG: Email detail processing failed: {e}")
                    import traceback
                    logger.error(f"âŒ DEBUG: Traceback: {traceback.format_exc()}")
                    special_response = f"ðŸ“§ **Email Detail Error**\n\nError: {str(e)}"
        
        elif is_draft_cmd:
            logger.info(f"âœ‰ï¸ DEBUG: Draft creation command detected")
            try:
                # Get conversation history for context
                history = await memory_manager.get_conversation_history(thread_id, limit=10)
                special_response = await process_draft_creation_command(history, user_id, draft_instruction)
                logger.info("âœ… DEBUG: Draft created successfully")
            except Exception as e:
                logger.error(f"âŒ DEBUG: Draft creation failed: {e}")
                import traceback
                logger.error(f"âŒ DEBUG: Traceback: {traceback.format_exc()}")
                special_response = f"âœ‰ï¸ **Draft Creation Error**\n\nError: {str(e)}"
        
        # 9b. ðŸ” Google Workspace command detection (AFTER email/draft) - 9/30/25
        elif detect_google_command(message_content)[0]:  # [0] gets the boolean from the tuple
            logger.info("ðŸ” DEBUG: Google Workspace command detected - processing...")
            try:
                logger.info(f"ðŸ” DEBUG: Calling process_google_command with user_id={user_id}")
                special_response = await process_google_command(message_content, user_id)
                logger.info("âœ… DEBUG: Google Workspace response generated successfully")
            except Exception as e:
                logger.error(f"âŒ DEBUG: Google Workspace processing failed: {e}")
                logger.error(f"âŒ DEBUG: Full exception details: {repr(e)}")
                import traceback
                logger.error(f"âŒ DEBUG: Traceback: {traceback.format_exc()}")
                special_response = f"ðŸ” **Google Workspace Processing Error**\n\nError: {str(e)}"
        
            
    # 10. ðŸ¥ Health Check command detection (NINTH)
        elif any(term in message_content.lower() for term in ['health check', 'system status', 'system health', 'how are you feeling']):
            logger.info("ðŸ¥ DEBUG: Health check command detected - processing...")
            try:
                from ..core.health import get_health_status
                health_data = await get_health_status()
                
                special_response = f"""ðŸ¥ **System Health Check**

**Overall Status:** {"âœ… Healthy" if health_data.get('healthy', False) else "âš ï¸ Issues Detected"}
**Database:** {"âœ… Connected" if health_data.get('database', {}).get('connected', False) else "âŒ Disconnected"}
**AI Brain:** {"âœ… Active" if health_data.get('ai_brain', {}).get('healthy', False) else "âš ï¸ Issues"}
**Integrations:** {health_data.get('active_integrations', 0)} active

**Response Time:** {health_data.get('response_time_ms', 0)}ms
**Memory Usage:** {health_data.get('memory_usage', {}).get('percent', 'N/A')}%
**Uptime:** {health_data.get('uptime', 'Unknown')}

All systems operational and ready to assist!"""
                logger.info("âœ… DEBUG: Health check response generated successfully")
            except Exception as e:
                logger.error(f"âŒ DEBUG: Health check processing failed: {e}")
                special_response = f"ðŸ¥ **Health Check Error**\n\nUnable to retrieve system health: {str(e)}"
        
        # 11. ðŸ§  Chat/AI function (TENTH - DEFAULT AI PROCESSING)
        if special_response:
            # Use the special response from one of the integrations
            logger.info(f"âœ… DEBUG: Using special integration response (length: {len(special_response)} chars)")
            final_response = special_response
        else:
            # Regular AI processing with full integration context
            logger.info("ðŸ§  DEBUG: Processing regular AI chat request...")
            
            try:
                # Get conversation history
                logger.info("ðŸ“š DEBUG: Getting conversation history...")
                conversation_history, context_info = await memory_manager.get_context_for_ai(
                    thread_id, max_tokens=20000
                )
                logger.info(f"âœ… DEBUG: Conversation history retrieved: {context_info.get('total_messages', 0)} messages")
                
                # Search knowledge base if requested
                if include_knowledge:
                    logger.info("ðŸ” DEBUG: Searching knowledge base...")
                    try:
                        knowledge_results = await knowledge_engine.search_knowledge(
                            query=message_content,
                            personality_id=personality_id,
                            limit=5
                        )
                        knowledge_sources = knowledge_results
                        logger.info(f"âœ… DEBUG: Knowledge search completed: {len(knowledge_sources)} sources found")
                    except Exception as e:
                        logger.error(f"âŒ DEBUG: Knowledge search failed: {e}")
                        knowledge_sources = []
                else:
                    logger.info("â­ DEBUG: Skipping knowledge search (disabled)")
                    knowledge_sources = []
                
                # Get RSS marketing context for writing assistance (integration #3)
                rss_context = ""
                if detect_rss_command(message_content) or any(term in message_content.lower() for term in ['write', 'content', 'marketing', 'blog', 'email']):
                    logger.info("ðŸ“° DEBUG: Adding RSS Learning context to AI response...")
                    try:
                        rss_context = await get_rss_marketing_context(message_content)
                        logger.info(f"âœ… DEBUG: RSS context retrieved (length: {len(rss_context)} chars)")
                    except Exception as e:
                        logger.error(f"âŒ DEBUG: RSS context failed: {e}")
                        rss_context = ""
                
                # Build system prompt with personality
                logger.info("ðŸŽ­ DEBUG: Building personality system prompt...")
                try:
                    personality_prompt = personality_engine.get_personality_system_prompt(
                        personality_id,
                        conversation_context=conversation_history
                    )
                    logger.info(f"âœ… DEBUG: Personality prompt generated (length: {len(personality_prompt)} chars)")
                except Exception as e:
                    logger.error(f"âŒ DEBUG: Personality prompt generation failed: {e}")
                    personality_prompt = "You are a helpful AI assistant."
                
                # Create enhanced system prompt with context
                system_parts = [
                    personality_prompt,
                    f"""Current DateTime Context: {datetime_context['full_datetime']}
Today is {datetime_context.get('day_of_week', 'Unknown')}, {datetime_context.get('month_name', 'Unknown')} {datetime_context.get('current_date', 'Unknown')}.
Current time: {datetime_context.get('current_time_12h', 'Unknown')} ({datetime_context.get('timezone', 'Unknown')})

User Context: The user is asking questions on {datetime_context['full_datetime']}.
When discussing time or dates, use the current information provided above.

Integration Status: All systems active - Weather, Bluesky, RSS Learning, Marketing Scraper, Prayer Times, Google Trends, Voice Synthesis, Image Generation, and Health monitoring are available via chat commands."""
                ]
                
                # Add RSS context if available
                if rss_context:
                    system_parts.append(rss_context)
                    logger.info("ðŸ“° DEBUG: RSS context added to system prompt")
                
                # Add knowledge context
                if knowledge_sources:
                    knowledge_context = "RELEVANT KNOWLEDGE BASE INFORMATION:\n"
                    for source in knowledge_sources:
                        knowledge_context += f"- {source['title']}: {source['content'][:200]}...\n"
                    system_parts.append(knowledge_context)
                    logger.info(f"ðŸ“š DEBUG: Knowledge context added: {len(knowledge_sources)} sources")
                
                # Build AI messages
                logger.info("ðŸ’¬ DEBUG: Building AI message array...")
                ai_messages = [{
                    "role": "system",
                    "content": "\n\n".join(system_parts)
                }]
                
                # Add conversation history
                ai_messages.extend(conversation_history)
                logger.info(f"âœ… DEBUG: AI messages array built: {len(ai_messages)} total messages")
                
                # Add current message
                ai_messages.append({
                    "role": "user",
                    "content": message_content
                })
                
                # Get AI response
                logger.info("ðŸ¤– DEBUG: Calling OpenRouter for AI response...")
                try:
                    ai_response = await openrouter_client.chat_completion(
                        messages=ai_messages,
                        model="anthropic/claude-3.5-sonnet:beta",
                        max_tokens=4000,
                        temperature=0.7
                    )
                    logger.info("âœ… DEBUG: OpenRouter response received")
                except Exception as e:
                    logger.error(f"âŒ DEBUG: OpenRouter call failed: {e}")
                    raise
                
                # Extract response content
                if ai_response and 'choices' in ai_response:
                    final_response = ai_response['choices'][0]['message']['content']
                    model_used = ai_response.get('model', 'claude-3.5-sonnet')
                    logger.info(f"âœ… DEBUG: AI response extracted (length: {len(final_response)} chars)")
                    
                    # Apply personality post-processing (includes pattern fatigue if available)
                    logger.info("ðŸŽ­ DEBUG: Applying personality post-processing...")
                    try:
                        personality_engine = get_personality_engine()
                        processed_response = personality_engine.process_personality_response(
                            final_response,
                            personality_id,
                            user_id
                        )
                        
                        if processed_response != final_response:
                            logger.info("ðŸš« DEBUG: Response modified by personality processing")
                            final_response = processed_response
                        else:
                            logger.info("âœ… DEBUG: Personality processing completed, no changes")
                    except Exception as e:
                        logger.warning(f"âš ï¸ DEBUG: Personality processing failed: {e}")
                        # Continue with original response if processing fails
            
            except Exception as e:
                logger.error(f"âŒ DEBUG: AI processing failed: {e}")
                raise
        
        # Calculate response time
        response_time_ms = int((time.time() - start_time) * 1000)
        logger.info(f"â±ï¸ DEBUG: Total processing time: {response_time_ms}ms")
        
        # Store AI response
        logger.info("ðŸ’¾ DEBUG: Storing AI response...")
        try:
            ai_message_id = await memory_manager.add_message(
                thread_id=thread_id,
                role="assistant",
                content=final_response,
                model_used=model_used,
                response_time_ms=response_time_ms,
                knowledge_sources_used=[source.get('id', '') for source in knowledge_sources]
            )
            logger.info(f"âœ… DEBUG: AI response stored: {ai_message_id}")
        except Exception as e:
            logger.error(f"âŒ DEBUG: Failed to store AI response: {e}")
            ai_message_id = str(uuid.uuid4())  # Fallback ID
        
        # Build response
        logger.info("ðŸ“¦ DEBUG: Building final response object...")
        
        try:
            chat_response = ChatResponse(
                response=final_response,
                thread_id=thread_id,
                message_id=ai_message_id,
                personality_id=personality_id,
                model_used=model_used,
                response_time_ms=response_time_ms,
                knowledge_sources=[
                    {
                        'id': source.get('id', ''),
                        'title': source.get('title', ''),
                        'snippet': source.get('content', '')[:200],
                        'score': source.get('score', 0.0)
                    }
                    for source in knowledge_sources
                ],
                conversation_context={
                    'thread_id': thread_id,
                    'message_count': context_info.get('total_messages', 0) + 1,
                    'has_knowledge': len(knowledge_sources) > 0,
                    'integration_processing_order': 'weather->bluesky->rss->scraper->prayer->google_trends->voice->image->health->ai'
                }
            )
            
            logger.info(f"âœ… CHAT SUCCESS: Response generated in {response_time_ms}ms")
            logger.info(f"ðŸ“Š DEBUG: Final stats - model: {model_used}, knowledge: {len(knowledge_sources)}, special: {bool(special_response)}")
            
            return chat_response
            
        except Exception as e:
            logger.error(f"âŒ DEBUG: Failed to build response object: {e}")
            raise
        
    except Exception as e:
        logger.error(f"âŒ CHAT FAILED: {str(e)}")
        logger.error(f"ðŸ’¥ DEBUG: Exception details:", exc_info=True)
        
        # Store error message
        error_message = f"Sorry, I encountered an error processing your message. Please try again.\n\nError details: {str(e)}"
        
        error_message_id = str(uuid.uuid4())
        response_time_ms = int((time.time() - start_time) * 1000)
        
        try:
            memory_manager = get_memory_manager(user_id)
            await memory_manager.add_message(
                thread_id=thread_id,
                role="assistant",
                content=error_message
            )
            logger.info("âœ… DEBUG: Error message stored successfully")
        except Exception as store_error:
            logger.error(f"âŒ DEBUG: Failed to store error message: {store_error}")
        
        return ChatResponse(
            response=error_message,
            thread_id=thread_id,
            message_id=error_message_id,
            personality_id=personality_id,
            model_used="error",
            response_time_ms=response_time_ms,
            knowledge_sources=[],
            conversation_context={'error': True, 'error_type': type(e).__name__}
        )

#-- Support Endpoints (NO DUPLICATION - CLEAN SUPPORT ONLY)

@router.post("/feedback", response_model=FeedbackResponse)
async def submit_feedback(feedback_request: FeedbackRequest):
    """Submit feedback for AI learning (ðŸ‘ðŸ‘ŽðŸ˜„)"""
    user_id = DEFAULT_USER_ID
    feedback_processor = get_feedback_processor()
    
    # Get thread_id from message
    try:
        from ..core.database import db_manager
        thread_lookup_query = """
        SELECT thread_id FROM conversation_messages 
        WHERE id = $1
        """
        message_result = await db_manager.fetch_one(thread_lookup_query, feedback_request.message_id)
        thread_id = message_result['thread_id'] if message_result else str(uuid.uuid4())
    except Exception as e:
        thread_id = str(uuid.uuid4())
        logger.warning(f"Could not lookup thread_id for message {feedback_request.message_id}: {e}")
    
    try:
        feedback_result = await feedback_processor.record_feedback(
            user_id=user_id,
            message_id=feedback_request.message_id,
            thread_id=thread_id,
            feedback_type=feedback_request.feedback_type,
            feedback_text=feedback_request.feedback_text
        )
        
        # Generate response message
        emoji = feedback_result['emoji']
        feedback_type = feedback_request.feedback_type
        
        if feedback_type == 'personality':
            message = f"{emoji} Perfect personality! I'll remember this energy."
        elif feedback_type == 'good':
            message = f"{emoji} Thanks! I'll reinforce this approach."
        elif feedback_type == 'bad':
            message = f"{emoji} Got it, I'll avoid this approach next time."
        else:
            message = f"{emoji} Feedback received."
        
        return FeedbackResponse(
            feedback_id=feedback_result['feedback_id'],
            feedback_type=feedback_result['feedback_type'],
            emoji=emoji,
            learning_result=feedback_result['learning_result'],
            message=message
        )
        
    except Exception as e:
        logger.error(f"Feedback processing failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/personalities")
async def get_personalities():
    """Get list of available personalities"""
    personality_engine = get_personality_engine()
    personalities = personality_engine.get_available_personalities()
    
    return {
        "personalities": [
            {
                "id": pid,
                "name": config.get('name', pid),
                "description": config.get('description', ''),
                "is_default": pid == 'syntaxprime'
            }
            for pid, config in personalities.items()
        ],
        "default_personality": "syntaxprime"
    }

@router.get("/conversations")
async def get_conversations(limit: int = 50):
    """Get conversation threads"""
    try:
        from ..core.database import db_manager
        conversations_query = """
        SELECT id, title, platform, status, message_count, 
               created_at, updated_at, last_message_at
        FROM conversation_threads
        WHERE user_id = $1
        ORDER BY last_message_at DESC
        LIMIT $2
        """
        threads = await db_manager.fetch_all(conversations_query, DEFAULT_USER_ID, limit)
        
        conversations = []
        for thread in threads:
            conversations.append({
                'thread_id': thread['id'],
                'title': thread['title'] or f"Conversation {thread['created_at'].strftime('%m/%d')}" if thread['created_at'] else 'New Conversation',
                'platform': thread['platform'],
                'status': thread['status'],
                'message_count': thread['message_count'] or 0,
                'created_at': thread['created_at'].isoformat() if thread['created_at'] else None,
                'updated_at': thread['updated_at'].isoformat() if thread['updated_at'] else None,
                'last_message_at': thread['last_message_at'].isoformat() if thread['last_message_at'] else None
            })
        
        return {
            "conversations": conversations,
            "total_available": len(conversations)
        }
        
    except Exception as e:
        logger.error(f"Failed to get conversations: {e}")
        return {"conversations": [], "total_available": 0}

@router.get("/conversations/{thread_id}")
async def get_conversation(thread_id: str, include_metadata: bool = False):
    """Get specific conversation history"""
    memory_manager = get_memory_manager(DEFAULT_USER_ID)
    
    try:
        messages = await memory_manager.get_conversation_history(
            thread_id,
            include_metadata=include_metadata
        )
        
        return {
            "thread_id": thread_id,
            "messages": messages,
            "message_count": len(messages)
        }
    except Exception as e:
        raise HTTPException(status_code=404, detail=f"Conversation not found: {e}")

@router.get("/knowledge/search")
async def search_knowledge(q: str, personality: str = 'syntaxprime', limit: int = 10):
    """Search knowledge base"""
    knowledge_engine = get_knowledge_engine()
    
    results = await knowledge_engine.search_knowledge(
        query=q,
        personality_id=personality,
        limit=limit
    )
    
    return {
        "query": q,
        "results": results,
        "count": len(results)
    }

@router.get("/stats")
async def get_ai_stats():
    """Get AI brain statistics"""
    memory_manager = get_memory_manager(DEFAULT_USER_ID)
    personality_engine = get_personality_engine()
    feedback_processor = get_feedback_processor()
    
    try:
        from ..core.database import db_manager
        
        conversation_stats_query = """
        SELECT 
            COUNT(DISTINCT ct.id) as total_conversations,
            COUNT(cm.id) as total_messages,
            COUNT(CASE WHEN cm.role = 'user' THEN 1 END) as user_messages,
            COUNT(CASE WHEN cm.role = 'assistant' THEN 1 END) as assistant_messages,
            AVG(cm.response_time_ms) as avg_response_time_ms,
            MAX(ct.last_message_at) as last_conversation_at
        FROM conversation_threads ct
        LEFT JOIN conversation_messages cm ON ct.id = cm.thread_id
        WHERE ct.user_id = $1
        """
        
        stats_result = await db_manager.fetch_one(conversation_stats_query, DEFAULT_USER_ID)
        personality_stats = personality_engine.get_personality_stats()
        feedback_summary = await feedback_processor.get_feedback_summary(DEFAULT_USER_ID)
        
        return {
            "conversation_stats": {
                "total_conversations": stats_result['total_conversations'] or 0,
                "total_messages": stats_result['total_messages'] or 0,
                "user_messages": stats_result['user_messages'] or 0,
                "assistant_messages": stats_result['assistant_messages'] or 0,
                "average_response_time_ms": float(stats_result['avg_response_time_ms']) if stats_result['avg_response_time_ms'] else 0,
                "last_conversation_at": stats_result['last_conversation_at'].isoformat() if stats_result['last_conversation_at'] else None
            },
            "personality_stats": personality_stats,
            "feedback_stats": feedback_summary,
            "integration_order": "weather->bluesky->rss->scraper->prayer->google_trends->voice->image->health->ai",
            "system_health": {
                "memory_active": True,
                "knowledge_engine_active": True,
                "learning_active": feedback_summary.get('learning_active', False),
                "all_integrations_active": True,
                "default_user_id": DEFAULT_USER_ID
            }
        }
        
    except Exception as e:
        logger.error(f"Stats retrieval failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/test")
async def test_ai_connection():
    """Test AI provider connections"""
    results = {}
    
    # Test OpenRouter
    try:
        from .openrouter_client import test_openrouter_connection
        results['openrouter'] = await test_openrouter_connection()
    except Exception as e:
        results['openrouter'] = {'status': 'error', 'message': str(e)}
    
    # Test Inception Labs
    try:
        inception_client = await get_inception_client()
        results['inception_labs'] = await inception_client.test_connection()
    except Exception as e:
        results['inception_labs'] = {'status': 'error', 'message': str(e)}
    
    # Overall health
    primary_healthy = results.get('openrouter', {}).get('status') == 'success'
    fallback_available = results.get('inception_labs', {}).get('status') in ['success', 'placeholder']
    
    return {
        "providers": results,
        "system_status": "healthy" if primary_healthy or fallback_available else "degraded",
        "primary_provider": "openrouter",
        "fallback_provider": "inception_labs",
        "integration_order": "weather->bluesky->rss->scraper->prayer->google_trends->voice->image->health->ai",
        "default_user_id": DEFAULT_USER_ID
    }

#-- Cleanup and Shutdown Handlers
@router.on_event("shutdown")
async def shutdown_ai_brain():
    """Cleanup AI brain components"""
    logger.info("Shutting down AI brain support components...")
    
    await cleanup_openrouter_client()
    await cleanup_inception_client()
    await cleanup_memory_managers()
    
    logger.info("AI brain support shutdown complete")

#-- Module Information Functions
def get_integration_info():
    """Get information about the AI brain router integration"""
    return {
        "name": "AI Brain Router with Full Integration Support + Voice & Image",
        "version": "2.2.0",
        "description": "Complete AI chat with ordered integration processing including Voice Synthesis and Image Generation",
        "integration_order": "ðŸŒ¦ï¸ Weather â†’ ðŸ”µ Bluesky â†’ ðŸ“° RSS â†’ ðŸ” Scraper â†’ ðŸ•Œ Prayer â†’ ðŸ“ˆ Google Trends â†’ ðŸŽ¤ Voice â†’ ðŸŽ¨ Image â†’ ðŸ¥ Health â†’ ðŸ§  Chat/AI",
        "components": [
            "Main Chat Endpoint (/ai/chat)",
            "Personality Engine",
            "Feedback Processor",
            "Conversation Manager",
            "Knowledge Query Engine",
            "AI Provider Testing",
            "Weather Integration",
            "Bluesky Social Media Management",
            "RSS Learning System",
            "Marketing Scraper",
            "Prayer Times with Notifications",
            "Location Detection",
            "Google Trends Analysis",
            "Voice Synthesis with ElevenLabs",
            "Image Generation with Replicate",
            "Health Monitoring"
        ],
        "endpoints": {
            "chat": "/ai/chat",
            "feedback": "/ai/feedback",
            "personalities": "/ai/personalities",
            "conversations": "/ai/conversations",
            "knowledge_search": "/ai/knowledge/search",
            "stats": "/ai/stats",
            "test": "/ai/test"
        },
        "features": [
            "ðŸ‘ðŸ‘ŽðŸ˜„ feedback learning",
            "ðŸŽ­ Multi-personality system",
            "ðŸ“š Knowledge base integration",
            "ðŸŒ¦ï¸ Weather monitoring",
            "ðŸ”µ Bluesky social management",
            "ðŸ“° RSS learning insights",
            "ðŸ” Marketing competitive analysis",
            "ðŸ•Œ Islamic prayer times with notifications",
            "ðŸ“ IP-based location detection",
            "ðŸ“ˆ Google Trends analysis",
            "ðŸŽ¤ Voice synthesis with personality voices",
            "ðŸŽ¨ AI image generation with inline display",
            "ðŸ¥ System health monitoring",
            "ðŸ§  Advanced AI chat processing",
            "ðŸ”§ Extensive debug logging"
        ],
        "debug_features": [
            "Step-by-step processing logs",
            "Integration command detection tracing",
            "Error context preservation",
            "Performance timing measurements",
            "Component loading verification",
            "Response generation tracking"
        ],
        "default_user_id": DEFAULT_USER_ID
    }

def check_module_health():
    """Check AI brain router module health"""
    missing_vars = []
    
    # Check required environment variables
    if not os.getenv("OPENROUTER_API_KEY"):
        missing_vars.append("OPENROUTER_API_KEY")
    
    return {
        "healthy": len(missing_vars) == 0,
        "missing_vars": missing_vars,
        "status": "ready" if len(missing_vars) == 0 else "needs_configuration",
        "chat_endpoint_active": True,
        "integration_order": "weather->bluesky->rss->scraper->prayer->google_trends->voice->image->health->ai",
        "debug_features_active": True,
        "voice_synthesis_integrated": True,
        "image_generation_integrated": True,
        "default_user_id": DEFAULT_USER_ID,
        "note": "Complete AI router with Voice & Image integrations properly ordered and extensive debugging"
    }
